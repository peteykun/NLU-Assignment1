{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import regex as re\n",
    "import random\n",
    "from collections import defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to /home/petey/nltk_data...\n",
      "[nltk_data]   Package gutenberg is already up-to-date!\n",
      "[nltk_data] Downloading package brown to /home/petey/nltk_data...\n",
      "[nltk_data]   Package brown is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/petey/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg, brown\n",
    "\n",
    "nltk.download('gutenberg')\n",
    "nltk.download('brown')\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_sentences = []\n",
    "\n",
    "for fileid in gutenberg.fileids():\n",
    "    gutenberg_sentences += list(gutenberg.sents(fileid))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "brown_sentences = brown.sents()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_sents(sents):\n",
    "    lines = [' '.join(x) for x in sents]\n",
    "\n",
    "    # Remove punctuation\n",
    "    lines = [re.sub(ur\"\\p{P}+\", \"\", line) for line in lines]\n",
    "\n",
    "    lines = [line.replace(\"'s \", \" 's/\") for line in lines]\n",
    "    lines = [' '.join([w.split('/')[0] for w in line.split(' ')]) for line in lines]\n",
    "\n",
    "    # Split hyphenated words\n",
    "    lines = [line.replace('-', ' ') for line in lines]\n",
    "\n",
    "    # Replace numbers\n",
    "    lines = [re.sub(\"\\s([\\d]+[\\d,\\.]*)(th)?\\s\", \" @@NUMBER@@ \", ' ' + line + ' ') for line in lines]\n",
    "    lines = [re.sub(\"\\s([\\d]+[\\d,\\.]*)(th)?\\s\", \" @@NUMBER@@ \", ' ' + line + ' ') for line in lines]\n",
    "\n",
    "    # Replace currency\n",
    "    lines = [re.sub(\"\\s([\\$]\\.?[\\d]+[\\d,\\.]*)\\s\", \" @@CURRENCY@@ \", ' ' + line + ' ') for line in lines]\n",
    "\n",
    "    # Add start and end symbols\n",
    "    lines = ['@@START@@ ' + line + ' @@END@@' for line in lines]\n",
    "\n",
    "    # Split\n",
    "    lines = [line.split(' ') for line in lines]\n",
    "\n",
    "    # Normalize case\n",
    "    lines = [[w.lower().strip() for w in line if w.strip() != ''] for line in lines]\n",
    "    \n",
    "    return lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_sentences = process_sents(gutenberg_sentences)\n",
    "brown_sentences = process_sents(brown_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(1337)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_train_idx = set(random.sample(range(len(gutenberg_sentences)), int(0.9*len(gutenberg_sentences))))\n",
    "gutenberg_valid_idx = set(range(len(gutenberg_sentences))) - gutenberg_train_idx\n",
    "\n",
    "brown_train_idx = set(random.sample(range(len(brown_sentences)), int(0.9*len(brown_sentences))))\n",
    "brown_valid_idx = set(range(len(brown_sentences))) - brown_train_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "gutenberg_train_sentences = [gutenberg_sentences[i] for i in gutenberg_train_idx]\n",
    "gutenberg_valid_sentences = [gutenberg_sentences[i] for i in gutenberg_valid_idx]\n",
    "\n",
    "brown_train_sentences = [brown_sentences[i] for i in brown_train_idx]\n",
    "brown_valid_sentences = [brown_sentences[i] for i in brown_valid_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_datasets = ['gutenberg', 'brown']\n",
    "test_datasets  = ['brown']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sentences = []\n",
    "valid_sentences = []\n",
    "\n",
    "if 'gutenberg' in train_datasets:\n",
    "    train_sentences += gutenberg_train_sentences\n",
    "if 'gutenberg' in test_datasets:\n",
    "    valid_sentences += gutenberg_valid_sentences\n",
    "if 'brown' in train_datasets:\n",
    "    train_sentences += brown_train_sentences\n",
    "if 'brown' in test_datasets:\n",
    "    valid_sentences += brown_valid_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract vocabulary\n",
    "vocab = set()\n",
    "frequency = dict()\n",
    "\n",
    "for sentence in train_sentences:\n",
    "    for word in sentence:\n",
    "        if word not in vocab:\n",
    "            vocab.add(word)\n",
    "            frequency[word] = 1\n",
    "        else:\n",
    "            frequency[word] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 39065\n"
     ]
    }
   ],
   "source": [
    "for word, freq in frequency.iteritems():\n",
    "    if freq < threshold:\n",
    "        if word in vocab:\n",
    "            vocab.remove(word)\n",
    "            \n",
    "print 'Vocabulary size:', len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build mapping from vocabulary to IDs\n",
    "word_to_id = dict()\n",
    "\n",
    "for word in vocab:\n",
    "    word_to_id[word] = len(word_to_id) + 1\n",
    "    \n",
    "id_to_word = dict([(id_, word) for word, id_ in word_to_id.iteritems()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store counts\n",
    "n_grams = dict()\n",
    "\n",
    "for n in range(1,2+1):\n",
    "    n_grams[n] = defaultdict(dict)\n",
    "    \n",
    "    for sentence in train_sentences:\n",
    "        for i in range(len(sentence) - n + 1):\n",
    "            key = tuple(map(lambda x: word_to_id[x] if x in word_to_id else 0, sentence[i:i+n]))\n",
    "            \n",
    "            if key in n_grams[n]:\n",
    "                n_grams[n][key] += 1\n",
    "            else:\n",
    "                n_grams[n][key] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix = '_'.join(train_datasets) + '-' + '_'.join(test_datasets)\n",
    "\n",
    "np.save('%s_word_to_id.npy' % prefix, word_to_id)\n",
    "np.save('%s_id_to_word.npy' % prefix, id_to_word)\n",
    "np.save('%s_n_grams.npy' % prefix, n_grams)\n",
    "np.save('%s_train.npy' % prefix, train_sentences)\n",
    "np.save('%s_valid.npy' % prefix, valid_sentences)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
